{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4W6L+v/BahONa5EGSeAom",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/el-eshaano/ml/blob/main/Transformers/Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KajLTPH7wuEl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 2500\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "embd_dims = 64\n",
        "n_heads = 4\n",
        "n_layers = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# ------------"
      ],
      "metadata": {
        "id": "Ng29najzwiQ9",
        "outputId": "e690fde2-521e-4e0d-803e-a45582cae03b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x785bab7a58d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR6Ulcx11El1",
        "outputId": "a3c03c21-7dc5-4088-86d1-de57c44e1b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-18 11:40:15--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.13’\n",
            "\n",
            "\rinput.txt.13          0%[                    ]       0  --.-KB/s               \rinput.txt.13        100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-18 11:40:15 (22.2 MB/s) - ‘input.txt.13’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "PQvhE4cI2CsB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQc7XW0i2cyl",
        "outputId": "e5e49361-029c-4a46-a6e1-54f603ec91b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s : [stoi[c] for c in s]\n",
        "decode = lambda n : ''.join([itos[d] for d in n])"
      ],
      "metadata": {
        "id": "m9VzeU0O2ub9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ],
      "metadata": {
        "id": "hMpyFz_w5UWI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Concept - `block_size`\n",
        "\n",
        "The `block_size` specifies the maximum size of the data we will pull for training. It is the maximum number of examples that will be in any subsection of the array that we pull. It represents the maximum context level of our model\n",
        "\n",
        "So, for example, suppose our array is:\n",
        "`[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
        "\n",
        "Here, if `block_size = 2`, we will pull 3 elements from the array, lets say the first three: `[1, 2, 3]`\n",
        "\n",
        "This set of 3 elements contains **two** training examples for our transformer,\n",
        "1. `1 => 2`\n",
        "2. `1, 2 => 3`\n",
        "\n",
        "Hence we will always take a subarray of size `block_size + 1`\n",
        "\n",
        "So we can write\n",
        "```python\n",
        "x = test_array[:block_size]\n",
        "y = test_array[1:block_size+1]\n",
        "```\n",
        "\n",
        "Hence, for any given value `t` in the `range(block_size)`\n",
        "```python\n",
        "context = x[:t+1]\n",
        "target = y[t]\n",
        "```\n"
      ],
      "metadata": {
        "id": "bJ47D_1h6Rl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split=\"train\"):\n",
        "    data = train_data if split == 'train' else test_data\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, )) # function call is torch.randint(low=0, high, size, ...)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "SD5j3XKx7HpO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super(SelfAttentionHead, self).__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "\n",
        "        self.queries = nn.Linear(embd_dims, head_size)\n",
        "        self.keys = nn.Linear(embd_dims, head_size)\n",
        "        self.values = nn.Linear(embd_dims, head_size)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.keys(x)\n",
        "        q = self.queries(x)\n",
        "\n",
        "        # print(k.shape, q.shape)\n",
        "\n",
        "        affs = q @ k.transpose(-2, -1) * C**-0.5 # Divide by C^(-0.5) so that affs has unit var\n",
        "        affs = affs.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Present should not be able to communicate with future\n",
        "        affs = F.softmax(affs, dim=-1) # Normalize\n",
        "        affs = self.dropout(affs)\n",
        "\n",
        "        v = self.values(x)\n",
        "        return affs @ v"
      ],
      "metadata": {
        "id": "QTE-Y_zIG8el"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.sa_heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(embd_dims, embd_dims)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.sa_heads], dim=-1) # Concat along channel dim\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "5DRInxyLxeIs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "\n",
        "        super(SingleFeedForward, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_size, out_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(out_size, in_size), # Project back so that it can be added with the residual pathway\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "Mgr_r5Pozi4-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embd_dims, n_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        head_size = embd_dims // n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "        self.ffwd = SingleFeedForward(embd_dims, 4 * embd_dims)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embd_dims)\n",
        "        self.ln2 = nn.LayerNorm(embd_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Skip connection with x added\n",
        "        x = x + self.ffwd(self.ln2(x)) # Another skip connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "f8W5Oav3w3EU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embd_dims)\n",
        "        self.position_embedding = nn.Embedding(block_size, embd_dims)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(embd_dims, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embd_dims) # layer norm after all the transformer stuff\n",
        "        self.lm_head = nn.Linear(embd_dims, vocab_size) # output head of language model\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            cropped_x = x[:, -block_size:] # Only take block size amount of context\n",
        "            logits, loss = self(cropped_x)\n",
        "            logits = logits[:, -1, :] # Only get last token for each batch\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_choice = torch.multinomial(probs, num_samples=1) # Predict the next charatcer given the probabilites\n",
        "            x = torch.concat((x, next_choice), dim=1)\n",
        "        self.train()\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eTWsdEza0gQP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderTransformer().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "zGsQc7M213rs",
        "outputId": "f8a7390f-6ea3-4a4b-f1bc-2b9703afc029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.210497 M parameters\n",
            "step 0: train loss 4.3090, val loss 4.3112\n",
            "step 100: train loss 2.6446, val loss 2.6524\n",
            "step 200: train loss 2.2733, val loss 2.2724\n",
            "step 300: train loss 1.6392, val loss 1.6524\n",
            "step 400: train loss 0.9465, val loss 0.9770\n",
            "step 500: train loss 0.5368, val loss 0.5856\n",
            "step 600: train loss 0.3680, val loss 0.4092\n",
            "step 700: train loss 0.2774, val loss 0.3049\n",
            "step 800: train loss 0.2189, val loss 0.2487\n",
            "step 900: train loss 0.1868, val loss 0.2246\n",
            "step 1000: train loss 0.1605, val loss 0.1813\n",
            "step 1100: train loss 0.1338, val loss 0.1570\n",
            "step 1200: train loss 0.1272, val loss 0.1423\n",
            "step 1300: train loss 0.1166, val loss 0.1315\n",
            "step 1400: train loss 0.1130, val loss 0.1250\n",
            "step 1500: train loss 0.1013, val loss 0.1150\n",
            "step 1600: train loss 0.1102, val loss 0.1201\n",
            "step 1700: train loss 0.1347, val loss 0.1461\n",
            "step 1800: train loss 0.0989, val loss 0.1085\n",
            "step 1900: train loss 0.1024, val loss 0.1128\n",
            "step 2000: train loss 0.1022, val loss 0.1137\n",
            "step 2100: train loss 0.1054, val loss 0.1094\n",
            "step 2200: train loss 0.0908, val loss 0.0987\n",
            "step 2300: train loss 0.0906, val loss 0.0945\n",
            "step 2400: train loss 0.1007, val loss 0.1090\n",
            "step 2499: train loss 0.0864, val loss 0.0911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "yv8oOVAz3R_B",
        "outputId": "7ea06426-89b4-4d7f-f166-6964770e6c30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mooooo horrinchhhcch raanath Ku aisen bobe toe.\n",
            "Sagr-'t theatitans bar bthie uhqurth bthar dinthoate arice mout fostatu zokou \n",
            "Youns-m'of in coing mit nditincueg ireens, hoin lat Hot duov te ande to poman'g trabe!\n",
            " lhin dome u.\n",
            "Hhuce courity:'tug haiss hiw yo nurin's normopete gods'sk:\n",
            "tink, titthakeno Winso whut eiings touti fouris so nuhireds poou gour; thu the hinteruf ff sor; igre! muf thin maleount ffaf Prisd mo om.\n",
            "WHKINLuk!\n",
            "Kuind isa\n",
            "ardsad this me sto fin couk ay andy iry tome fo mo vouck no tounke mary.\n",
            "Tou 'not buthm so aten hin.\n",
            "Wpour bethe thimreand shoun be hes th thutisod,.\n",
            "Butuch fosomy sstuth sou.\n",
            " moun sof thupeings.\n",
            "Whuespues she oveve imd assce oros ifk ovet soie so urd histe feRil ass:\n",
            "Whit CINGghatk nike, n idu he neesoxt atou thitheakes agh'scour\n",
            "'ss m k o with selon mo dous flla nomur ipatrcessingsughild.\n",
            "MLurd po ton'en'tun thity feo theoou tiund tho nof the sut pe iportheou suth than tthe silse, Bloel ke soun thy a thef bede wat hotiu, cout ir foru;\n",
            "youkgeg oueres tan in bofo. fasit fet Hink che whin thoui hod ser thitano nom notay'u brutond.\n",
            "Me, mess thinds sofee woriingin cou touh gre Whin ste ndichca noure goed hice nI irstha thein cou fas ind- an binketit blamenat mer fand eing biwimngiten hawe e wo I dinctod dot hid'ull hie\n",
            "tham Tuul lalou! trameolircoust jou' f f toupthriu thhuf thing mastin nit ih stinge.\n",
            "WPthrithth hout sove chuths beer sef o pvonbust thay take thit be sinn'th I thit e nu toum Thime te you thel!ie, ins ye you,\n",
            "But row ulat we\n",
            "erinst sisokesthouk pte-mer, sates souf for thouk vid scee Coun!t mouete pined sef aring stis mo en sacourse cok derk hiskn won thik.\n",
            "L\n",
            "'congdescoon rould. ar tith arit, gadtinght couf phit, hioass o'ung,'t these haw Helhern Wend tig fafl ith sup that heacruces citt, ofamk, liceoulk tand muak rite wincof fota sa I orat burs bliteerosevelw hig 'puo' foour sito the ain's ku ngou'tens seoury pu sous med! wonse tho tan dtus be of to, J shou pat ingtourkin shoun'edfgoun dme su me our hihen ke!\n",
            "M\n",
            "Mair\n"
          ]
        }
      ]
    }
  ]
}