{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4W6L+v/BahONa5EGSeAom",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/el-eshaano/ml/blob/main/Transformers/Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KajLTPH7wuEl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 2500\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "embd_dims = 64\n",
        "n_heads = 4\n",
        "n_layers = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# ------------"
      ],
      "metadata": {
        "id": "Ng29najzwiQ9",
        "outputId": "a378ad46-fa02-4eea-8276-bb1b0974494a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f317b7b95f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR6Ulcx11El1",
        "outputId": "f839f1b8-c8ce-4ca1-efb4-fc41791b290a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-26 08:36:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-26 08:36:00 (19.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "PQvhE4cI2CsB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQc7XW0i2cyl",
        "outputId": "ce5b7c2a-9e30-4582-de6f-ff78bfe66362"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s : [stoi[c] for c in s]\n",
        "decode = lambda n : ''.join([itos[d] for d in n])"
      ],
      "metadata": {
        "id": "m9VzeU0O2ub9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ],
      "metadata": {
        "id": "hMpyFz_w5UWI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Concept - `block_size`\n",
        "\n",
        "The `block_size` specifies the maximum size of the data we will pull for training. It is the maximum number of examples that will be in any subsection of the array that we pull. It represents the maximum context level of our model\n",
        "\n",
        "So, for example, suppose our array is:\n",
        "`[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
        "\n",
        "Here, if `block_size = 2`, we will pull 3 elements from the array, lets say the first three: `[1, 2, 3]`\n",
        "\n",
        "This set of 3 elements contains **two** training examples for our transformer,\n",
        "1. `1 => 2`\n",
        "2. `1, 2 => 3`\n",
        "\n",
        "Hence we will always take a subarray of size `block_size + 1`\n",
        "\n",
        "So we can write\n",
        "```python\n",
        "x = test_array[:block_size]\n",
        "y = test_array[1:block_size+1]\n",
        "```\n",
        "\n",
        "Hence, for any given value `t` in the `range(block_size)`\n",
        "```python\n",
        "context = x[:t+1]\n",
        "target = y[t]\n",
        "```\n"
      ],
      "metadata": {
        "id": "bJ47D_1h6Rl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split=\"train\"):\n",
        "    data = train_data if split == 'train' else test_data\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, )) # function call is torch.randint(low=0, high, size, ...)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "SD5j3XKx7HpO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super(SelfAttentionHead, self).__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "\n",
        "        self.queries = nn.Linear(embd_dims, head_size)\n",
        "        self.keys = nn.Linear(embd_dims, head_size)\n",
        "        self.values = nn.Linear(embd_dims, head_size)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.keys(x)\n",
        "        q = self.queries(x)\n",
        "\n",
        "        # print(k.shape, q.shape)\n",
        "\n",
        "        affs = q @ k.transpose(-2, -1) * C**-0.5 # Divide by C^(-0.5) so that affs has unit var\n",
        "        affs = affs.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Present should not be able to communicate with future\n",
        "        affs = F.softmax(affs, dim=-1) # Normalize\n",
        "        affs = self.dropout(affs)\n",
        "\n",
        "        v = self.values(x)\n",
        "        return affs @ v"
      ],
      "metadata": {
        "id": "QTE-Y_zIG8el"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.sa_heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(embd_dims, embd_dims)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.sa_heads], dim=-1) # Concat along channel dim\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "5DRInxyLxeIs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "\n",
        "        super(SingleFeedForward, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_size, out_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(out_size, in_size), # Project back so that it can be added with the residual pathway\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "Mgr_r5Pozi4-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embd_dims, n_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        head_size = embd_dims // n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "        self.ffwd = SingleFeedForward(embd_dims, 4 * embd_dims)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embd_dims)\n",
        "        self.ln2 = nn.LayerNorm(embd_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Skip connection with x added\n",
        "        x = x + self.ffwd(self.ln2(x)) # Another skip connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "f8W5Oav3w3EU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embd_dims)\n",
        "        self.position_embedding = nn.Embedding(block_size, embd_dims)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(embd_dims, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embd_dims) # layer norm after all the transformer stuff\n",
        "        self.lm_head = nn.Linear(embd_dims, vocab_size) # output head of language model\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            cropped_x = x[:, -block_size:] # Only take block size amount of context\n",
        "            logits, loss = self(cropped_x)\n",
        "            logits = logits[:, -1, :] # Only get last token for each batch\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_choice = torch.multinomial(probs, num_samples=1) # Predict the next charatcer given the probabilites\n",
        "            x = torch.concat((x, next_choice), dim=1)\n",
        "        self.train()\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eTWsdEza0gQP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderTransformer().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "zGsQc7M213rs",
        "outputId": "bc66a87f-87cb-483f-b909-c85f68bedcb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.210497 M parameters\n",
            "step 0: train loss 4.3172, val loss 4.3193\n",
            "step 100: train loss 2.6664, val loss 2.6741\n",
            "step 200: train loss 2.5124, val loss 2.5080\n",
            "step 300: train loss 2.4255, val loss 2.4227\n",
            "step 400: train loss 2.3517, val loss 2.3600\n",
            "step 500: train loss 2.3010, val loss 2.3139\n",
            "step 600: train loss 2.2526, val loss 2.2659\n",
            "step 700: train loss 2.2132, val loss 2.2232\n",
            "step 800: train loss 2.1697, val loss 2.2014\n",
            "step 900: train loss 2.1329, val loss 2.1651\n",
            "step 1000: train loss 2.0987, val loss 2.1377\n",
            "step 1100: train loss 2.0795, val loss 2.1252\n",
            "step 1200: train loss 2.0419, val loss 2.0849\n",
            "step 1300: train loss 2.0228, val loss 2.0677\n",
            "step 1400: train loss 1.9882, val loss 2.0452\n",
            "step 1500: train loss 1.9530, val loss 2.0232\n",
            "step 1600: train loss 1.9401, val loss 2.0207\n",
            "step 1700: train loss 1.9229, val loss 2.0074\n",
            "step 1800: train loss 1.9117, val loss 2.0111\n",
            "step 1900: train loss 1.8835, val loss 1.9733\n",
            "step 2000: train loss 1.8744, val loss 1.9854\n",
            "step 2100: train loss 1.8545, val loss 1.9593\n",
            "step 2200: train loss 1.8545, val loss 1.9567\n",
            "step 2300: train loss 1.8226, val loss 1.9357\n",
            "step 2400: train loss 1.8232, val loss 1.9342\n",
            "step 2499: train loss 1.7915, val loss 1.9266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "yv8oOVAz3R_B",
        "outputId": "6ea8802a-3547-49e5-84ce-b422ffe1ec9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And they brid own, which yest madier,\n",
            "More uperanthruans the alause: andt he usqurthert?\n",
            "Fed lascanesswick, you, not to zonounenouns, toffing I condriend;\n",
            "Whoue miree, bodcintlation, drovers, and Will.\n",
            "\n",
            "DUCHEWI OF ICH:\n",
            "Thate here she couriby ature aids hiw year's nourn Boopenclands\n",
            "Morthy world thake of in on her piicks eyer\n",
            "For thou we his now pooctor his butter of thrupted so;\n",
            "Anging muftenter alepon, for dear?\n",
            "\n",
            "KISCING HENRY E!\n",
            "\n",
            "HONb&st:\n",
            "Sads beaces and this hence as ranny it-pramence you!\n",
            "\n",
            "JOLn:\n",
            "To Pood, grivent 'What me?\n",
            " so a and see-ennom by weath mrean?\n",
            "\n",
            "BUCKINIO:\n",
            "Go no weep ower it under some sto-thas lord\n",
            "I ias we lore at thou eyeded heavoven:\n",
            "He passt to ousink over soie so upon sir!\n",
            "Fespeakes: the go, agure them, wis deeder.\n",
            "For mo more himbed with\n",
            "For vince war kis wiffes low moder so las noothing,\n",
            "These In ammildeminer, poor's in now this make I am it is\n",
            "oftherningn eysuran wip.\n",
            "\n",
            "GLOUCE-BOREN OF DORK:\n",
            "And, Berelike, of so hat the abacch\n",
            "atch time wout if for Gly thee? our Therefore\n",
            "To what I coret Henk; he when the cordook,\n",
            "And take all nobly hoor, and men, meen thinks and endersingin in-vinchigh him a word,\n",
            "Do caless and Edwance aInd my a canint and as incomen be keep,\n",
            "To me atteringry ofing brom not own\n",
            "aw-cepunce, if tha de't a for Howe\n",
            "thamper lows op!' the of thee tre's\n",
            "He fring throught?\n",
            "\n",
            "SCAREOLIO:\n",
            "So nir is cappressletir Wend's a'd your murds be:\n",
            "Hand of vone, the to them the bance hoble;\n",
            "And the namty may'd fore you thelpie,\n",
            "And yeed thes arrow old toe\n",
            "epingm sin just wuke them wars, eves;\n",
            "Rich, thouke--I scann:\n",
            "O there, And wherefore\n",
            "Op ristile bres wach preicy him kLet in well there?\n",
            "'Twis may man word. Pantler as I was thather\n",
            "Marmphir, byoans o' no, hithese haw an\n",
            "her of no hig wifle,\n",
            "But with I heapry, incild, of monguer.\n",
            "\n",
            "GLOUCENTES:\n",
            "Freper's befaint, say of at but the peepoor: like he punint\n",
            "What is a for would then of an a would prour,\n",
            "They aw-nsent, Jow hath and of a\n",
            "one sto woak's it brif and whom for atheer,\n",
            "Ame offer henwing, and I\n"
          ]
        }
      ]
    }
  ]
}