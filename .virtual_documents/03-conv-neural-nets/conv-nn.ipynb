import torch

import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F

from PIL import Image

from tqdm.notebook import tqdm

import matplotlib.pyplot as plt
plt.style.use("seaborn-v0_8")


transform = transforms.ToTensor()

mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

data_loading_params = {'batch_size': 16,
                       'shuffle': True,
                       'num_workers': 6
                      }

train_data = torch.utils.data.DataLoader(mnist_trainset, **data_loading_params)
test_data = torch.utils.data.DataLoader(mnist_testset, **data_loading_params)


class ConvNN(nn.Module):
    
    def __init__(self, channels, kernels, layers, learning_rate=0.001):
        super(ConvNN, self).__init__()
        
        self.conv = nn.ModuleList([nn.Conv2d(c, channels[i+1], k) for (i, c), k in zip(enumerate(channels[:-1]), kernels)])
        self.linear = nn.ModuleList([nn.Linear(v, layers[i+1]) for i, v in enumerate(layers[:-1])])
        self.pool = nn.MaxPool2d(2, 2)
        self.softmax = nn.Softmax(dim=1)

        self.criterion = nn.CrossEntropyLoss().to(device)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
    
    
    def forward(self, x):
        for i, l in enumerate(self.conv):
            x = self.pool(
                F.relu(
                l(x)
            ))
        
        x = torch.flatten(x, 1)

        for i, l in enumerate(self.linear[:-1]):
            x = F.relu(l(x))

        x = self.softmax(self.linear[-1](x))

        return x

    def fit(self, train_data, test_data, num_epochs=10):

        for epoch in tqdm(range(num_epochs), desc="Epochs", leave=False):
            for local_batch, local_labels in tqdm(train_data, desc="Batches", leave=False):
    
                local_batch, local_labels = local_batch.to(device), local_labels.to(device)
                
                self.optimizer.zero_grad()
                loss = self.criterion(self(local_batch), local_labels)
    
                loss.backward()
                self.optimizer.step()

    def calculate_accuracy(self, data_loader):
        correct = 0
        total = 0
    
        with torch.no_grad():
            for images, labels in data_loader:
                images, labels = images.to(device), labels.to(device)
    
                outputs = self(images)
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
    
        # Calculate accuracy
        accuracy = 100 * correct / total
        return accuracy
            
        


device = "cuda" if torch.cuda.is_available() else "cpu"

model = ConvNN(
    channels=[1, 5, 3], 
    kernels=[5, 3], 
    layers=[75, 25, 10], 
    learning_rate=0.001
).to(device)

model.fit(train_data, test_data)



print(model.calculate_accuracy(train_data), model.calculate_accuracy(test_data))
