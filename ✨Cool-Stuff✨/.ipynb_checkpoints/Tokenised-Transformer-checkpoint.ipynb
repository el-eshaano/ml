{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f4264e0-31bf-4a0b-8cbb-119958f96364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8059c334-9f21-437d-a31b-3330f895dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Shakespeare/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdb1fb-095c-4fac-9eb5-d5dec2d5203e",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca66dd4-602f-4719-af0c-5cb4467f8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_counts(tokens):\n",
    "\n",
    "    pair_counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "def replace_pair(arr, pair, replacement):\n",
    "\n",
    "    new_arr = []\n",
    "    i = 0\n",
    "    while i < len(arr):\n",
    "        if i < len(arr) - 1 and arr[i] == pair[0] and arr[i+1] == pair[1]:\n",
    "            new_arr.append(replacement)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_arr.append(arr[i])\n",
    "            i += 1\n",
    "\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4050e1d4-5b8c-4dc1-963b-9781bd598729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokeniser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\" given ids (list of integers), return Python string \"\"\"\n",
    "        tokens = \"\".join(self.vocab[idx] for idx in ids)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, string):\n",
    "        toks = [self.stoi[x] for x in string]\n",
    "        while len(toks) >= 2: # avoid error when only 1 token is passed\n",
    "            pair_counts = get_pair_counts(toks)\n",
    "            pair = min(pair_counts, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break # no more possible merges\n",
    "\n",
    "            new_token = self.merges[pair]\n",
    "            toks = replace_pair(toks, pair, new_token)\n",
    "        return toks\n",
    "\n",
    "    def train(self, text, desired_vocab_size):\n",
    "        unique_chars = sorted(list(set(text)))\n",
    "        self.current_vocab_size = len(unique_chars)\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(unique_chars) }\n",
    "        self.vocab = { i:ch for i,ch in enumerate(unique_chars) }\n",
    "\n",
    "        tokens = [self.stoi[x] for x in list(text)]\n",
    "\n",
    "        num_merges = desired_vocab_size - self.current_vocab_size\n",
    "        ids = list(tokens)\n",
    "\n",
    "        for i in tqdm(range(num_merges), desc=\"Merges: \", leave=False):\n",
    "            pair_counts = get_pair_counts(ids)\n",
    "            max_occuring_pair = max(pair_counts, key=pair_counts.get)\n",
    "            \n",
    "            if pair_counts[max_occuring_pair] == 1:\n",
    "                print(\"Max pair only occurs once, breaking from loop\")\n",
    "                break\n",
    "\n",
    "            new_token_code = self.current_vocab_size + i\n",
    "            ids = replace_pair(ids, max_occuring_pair, new_token_code)\n",
    "            self.merges[max_occuring_pair] = new_token_code\n",
    "        \n",
    "        for (p0, p1), idx in self.merges.items(): # ordered dictionaries baby (ly python >= 3.8)\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "\n",
    "        print(f\"Completed, with compression ratio: {len(tokens) / len(ids)}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c980def-6c52-46f4-9fbb-41f4e7f27658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merges:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, with compression ratio: 1.4957015446547222x\n"
     ]
    }
   ],
   "source": [
    "bpe = BPETokeniser()\n",
    "bpe.train(text, desired_vocab_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ddd54-905f-48d8-86e8-c05f38f1d279",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4708518a-9858-47f9-85d7-6bcc77438d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(bpe.encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e989d8e2-43cf-4500-be95-68b597c33148",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9713a3b1-538e-4e5d-97d0-ef1db0ff546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split=\"train\"):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # function call is torch.randint(low=0, high, size, ...)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a5c61fd-97ab-4ede-ad2b-0615c7d6fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5a12c-007e-453e-ac2f-f0a2692a38ec",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f213b363-0462-4bd6-b8a4-1bcdd0ecc700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8e940d4310>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "embd_dims = 128\n",
    "n_heads = 8\n",
    "n_layers = 8\n",
    "dropout = 0.3\n",
    "vocab_size = 128\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89b4395d-4f39-4a94-8cac-29d5eb8da11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super(SelfAttentionHead, self).__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.queries = nn.Linear(embd_dims, head_size)\n",
    "        self.keys = nn.Linear(embd_dims, head_size)\n",
    "        self.values = nn.Linear(embd_dims, head_size)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.keys(x)\n",
    "        q = self.queries(x)\n",
    "\n",
    "        # print(k.shape, q.shape)\n",
    "\n",
    "        affs = q @ k.transpose(-2, -1) * C**-0.5 # Divide by C^(-0.5) so that affs has unit var\n",
    "        affs = affs.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Present should not be able to communicate with future\n",
    "        affs = F.softmax(affs, dim=-1) # Normalize\n",
    "        affs = self.dropout(affs)\n",
    "\n",
    "        v = self.values(x)\n",
    "        return affs @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a1e202a-f11c-4ee5-a20a-f3e7230b59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.sa_heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embd_dims, embd_dims)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.sa_heads], dim=-1) # Concat along channel dim\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9b71c97-1157-42c9-af5b-2c100fb2574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        super(SingleFeedForward, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_size, in_size), # Project back so that it can be added with the residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6fccb500-3610-40ec-bc8b-b1179927b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embd_dims, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        head_size = embd_dims // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = SingleFeedForward(embd_dims, 4 * embd_dims)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embd_dims)\n",
    "        self.ln2 = nn.LayerNorm(embd_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # Skip connection with x added\n",
    "        x = x + self.ffwd(self.ln2(x)) # Another skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a4e0caf-ac9d-44f2-88bf-e73e7dfb54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embd_dims)\n",
    "        self.position_embedding = nn.Embedding(block_size, embd_dims)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embd_dims, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embd_dims) # layer norm after all the transformer stuff\n",
    "        # self.lm_head = nn.Linear(embd_dims, vocab_size) # output head of language model\n",
    "        self.lm_head = nn.Sequential(\n",
    "            nn.Linear(embd_dims, 50),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Linear(50, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            cropped_x = x[:, -block_size:] # Only take block size amount of context\n",
    "            logits, loss = self(cropped_x)\n",
    "            logits = logits[:, -1, :] # Only get last token for each batch\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_choice = torch.multinomial(probs, num_samples=1) # Predict the next charatcer given the probabilites\n",
    "            x = torch.concat((x, next_choice), dim=1)\n",
    "        self.train()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "455fe1d9-901c-473e-8f73-c720b06bacd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62244 M parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d03ef54ac314f19a8a5297eed9c3103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8537, val loss 4.8593\n",
      "step 100: train loss 4.0669, val loss 4.0517\n",
      "step 200: train loss 3.8879, val loss 3.8614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DecoderTransformer().to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "for iter in tqdm(range(max_iters), desc=\"Epoch: \"):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "plt.plot(range(max_iters), epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71328041-cb5e-43dc-992e-e9f7198c6d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "By hefore in the bukew of thou thour milk, and Ray'd fal\n",
      "sould lege.\n",
      "\n",
      "SICGILIA:\n",
      "There eat fass, that you, det ubver of your pein\n",
      "done! What to no lord.\n",
      "Come, Prookaceward my testal Secan oO I athe!\n",
      "\n",
      "KING EDHE:\n",
      "Bus wrought had he most will it not of hinould,\n",
      "Ay more up it Canant in I may oftwards his yescover:\n",
      "For no here heavanks plast, I am I grest wound\n",
      "And canrold Praceens content do'd wit; the may unter?\n",
      "\n",
      "VILLANG:\n",
      "you dear a that force adgay, hour honour fill\n",
      "no majourd upt it hald. Plord, eat and lack my are,\n",
      "Who repans, and da clins!\n",
      "\n",
      "DUKE 'TIO:\n",
      "I hath that gint to Harrown.\n",
      "\n",
      "CLADIUS:\n",
      "Sit Lave I am goits?\n",
      "\n",
      "LIXENBEE:\n",
      "Will fight the plander, I may requifect inch with a hanry dems,\n",
      "I shall po most asommend, to be full commnest hast my hape.\n",
      "And any lowat He by the Mackenter.\n",
      "\n",
      "PLANGE:\n",
      "Cerlow:\n",
      "Pige, a Thou one the gegingment entleen.\n",
      "I'll noble not for beward,ing thour to thesey will.\n",
      "\n",
      "LUCENTIO:\n",
      "I city the pitch is atchanten: shall and wrong.\n",
      "Cake beith ay upon to cannot your ades:\n",
      "And that well not in the Eventy to the pontredenty?\n",
      "\n",
      "MENIUS:\n",
      "I genel clease in to be fulieent'd menced would murs:\n",
      "No not to hracy been know so king?\n",
      "\n",
      "JULIA:\n",
      "Than in the lord, and irice it an Harrab plriod noter:\n",
      "VOn'llop. Cawh's wizen nale, lorg.\n",
      "\n",
      "OUCKINGHUM:\n",
      "Alk in shors, I thy tize forguit at and them bast dremp;\n",
      "The bany beck'd by to hims yeck, where not eard dantly pattener:\n",
      "If have pock hamband and angeles, in in huss,\n",
      "Amesteant fis them of rady on the to he wanshtsingread:\n",
      "I kno feasess fallil, sty!\n",
      "Which's plove, And comming your bur alefans, father.\n",
      "\n",
      "FRIOR:\n",
      "Here may tack exciousther but of soelthers,\n",
      "But ankey it, all bestitfed wherese: callsh: thou seeees afes:\n",
      "I me a gitter: ale-ent to grot-some and you?\n",
      "Nor not for that play detor rong tic the Guce him sheak'ds\n",
      "Upect be man I wold stlemner vatils and hand\n",
      "Gace for that Haly kingmaly popee,\n",
      "I have the pace any prosheld of simeel my suck.\n",
      "\n",
      "FRIG RICHARD:\n",
      "Says bay\n",
      "Neelk a wall be if saze her be and piressauly dother,\n",
      "By mirst vext of any y sentlajeslefy'd fare treat\n",
      "They roler? with sword; hater with all,\n",
      "In a prahy proffule, these her will? for this somellower while plate;\n",
      "For so set not must a beol to pleash and.\n",
      "\n",
      "CAGILLO:\n",
      "His you haxply love it Hard, I'll is thee of a\n",
      "Dearnesteral a habdle to my plold dite alkie.\n",
      "\n",
      "ROMEO:\n",
      "Herean so  would no sather,\n",
      "Thor goods a deving the quad cery lages,\n",
      "And Lot is taure of a cannot make and with serve,\n",
      "For ol:\n",
      "Peaacter's the laging your bajegn yours.\n",
      "\n",
      "KING EDWARD II:\n",
      "Calence it lest trunell; theyd, lp;\n",
      "by peent revoather spodviant sBoused.\n",
      "\n",
      "KING EDIKY\n",
      "MHakingH of ELARD VILI:\n",
      "Hen, fullizen citle of dongent thought seem's have.\n",
      "\n",
      "RARCENIUS:\n",
      "Neech,\n",
      "I did the sbice, cracde the is my wind;\n",
      "Thou, pottion she deeft honguect printres.\n",
      "\n",
      "YORK:\n",
      "Here with a thno, be I crats\n",
      "Than my want god With altice to smake and tand's words hat,\n",
      "in the retroed?\n",
      "\n",
      "ENNGPEY:\n",
      "Thy lad not that Glaces the sctour'd\n",
      "Will, you lit of this your masterly my for to to of his pons,\n",
      "What what my volder! Bo; in the gocel; so'\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(bpe.decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd735411-fdd1-400c-ba40-fd11031fbccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
